{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8971041,"sourceType":"datasetVersion","datasetId":5400819},{"sourceId":33551,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":28083}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install -U trl \n%pip install -U bitsandbytes \n%pip install -U wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-16T23:14:34.716612Z","iopub.execute_input":"2024-07-16T23:14:34.716869Z","iopub.status.idle":"2024-07-16T23:16:18.603769Z","shell.execute_reply.started":"2024-07-16T23:14:34.716847Z","shell.execute_reply":"2024-07-16T23:16:18.602626Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format","metadata":{"execution":{"iopub.status.busy":"2024-07-16T23:17:08.310668Z","iopub.execute_input":"2024-07-16T23:17:08.311024Z","iopub.status.idle":"2024-07-16T23:17:28.038473Z","shell.execute_reply.started":"2024-07-16T23:17:08.310991Z","shell.execute_reply":"2024-07-16T23:17:28.037437Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-07-16 23:17:15.292840: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-16 23:17:15.292945: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-16 23:17:15.425217: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format","metadata":{"execution":{"iopub.status.busy":"2024-07-16T23:18:09.404153Z","iopub.execute_input":"2024-07-16T23:18:09.404920Z","iopub.status.idle":"2024-07-16T23:18:09.410226Z","shell.execute_reply.started":"2024-07-16T23:18:09.404887Z","shell.execute_reply":"2024-07-16T23:18:09.409319Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n\nlogin(token = hf_token)\n\nwb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune Llama 3 8B on chess dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T23:22:39.767214Z","iopub.execute_input":"2024-07-16T23:22:39.767898Z","iopub.status.idle":"2024-07-16T23:23:00.010882Z","shell.execute_reply.started":"2024-07-16T23:22:39.767867Z","shell.execute_reply":"2024-07-16T23:23:00.009696Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkrishnaharshith1234\u001b[0m (\u001b[33mkrishnaharshith1234-bits-pilani\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240716_232242-gzrcqg9p</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/krishnaharshith1234-bits-pilani/Fine-tune%20Llama%203%208B%20on%20chess%20dataset/runs/gzrcqg9p' target=\"_blank\">deft-sun-1</a></strong> to <a href='https://wandb.ai/krishnaharshith1234-bits-pilani/Fine-tune%20Llama%203%208B%20on%20chess%20dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/krishnaharshith1234-bits-pilani/Fine-tune%20Llama%203%208B%20on%20chess%20dataset' target=\"_blank\">https://wandb.ai/krishnaharshith1234-bits-pilani/Fine-tune%20Llama%203%208B%20on%20chess%20dataset</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/krishnaharshith1234-bits-pilani/Fine-tune%20Llama%203%208B%20on%20chess%20dataset/runs/gzrcqg9p' target=\"_blank\">https://wandb.ai/krishnaharshith1234-bits-pilani/Fine-tune%20Llama%203%208B%20on%20chess%20dataset/runs/gzrcqg9p</a>"},"metadata":{}}]},{"cell_type":"code","source":"base_model = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\ndataset_name = \"/kaggle/input/finetuningchess/Fine_tuning_Dataset.csv\"\nnew_model = \"llama-3-8b-chess-chat\"","metadata":{"execution":{"iopub.status.busy":"2024-07-16T23:25:57.224386Z","iopub.execute_input":"2024-07-16T23:25:57.224760Z","iopub.status.idle":"2024-07-16T23:25:57.230469Z","shell.execute_reply.started":"2024-07-16T23:25:57.224731Z","shell.execute_reply":"2024-07-16T23:25:57.229396Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"torch_dtype = torch.float16\nattn_implementation = \"eager\"","metadata":{"execution":{"iopub.status.busy":"2024-07-16T23:26:23.206980Z","iopub.execute_input":"2024-07-16T23:26:23.207692Z","iopub.status.idle":"2024-07-16T23:26:23.213191Z","shell.execute_reply.started":"2024-07-16T23:26:23.207660Z","shell.execute_reply":"2024-07-16T23:26:23.212100Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T23:26:44.910216Z","iopub.execute_input":"2024-07-16T23:26:44.910716Z","iopub.status.idle":"2024-07-16T23:28:31.624006Z","shell.execute_reply.started":"2024-07-16T23:26:44.910685Z","shell.execute_reply":"2024-07-16T23:28:31.623077Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e749ef8ffbe1432ba6517726531f5c3b"}},"metadata":{}}]},{"cell_type":"code","source":"# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel, tokenizer = setup_chat_format(model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T23:29:05.949033Z","iopub.execute_input":"2024-07-16T23:29:05.949831Z","iopub.status.idle":"2024-07-16T23:29:06.452567Z","shell.execute_reply.started":"2024-07-16T23:29:05.949797Z","shell.execute_reply":"2024-07-16T23:29:06.451396Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T23:30:03.317062Z","iopub.execute_input":"2024-07-16T23:30:03.317912Z","iopub.status.idle":"2024-07-16T23:30:04.185430Z","shell.execute_reply.started":"2024-07-16T23:30:03.317877Z","shell.execute_reply":"2024-07-16T23:30:04.184446Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\n\n# Load the uploaded dataset\nfile_path = '/kaggle/input/finetuningchess/Fine_tuning_Dataset.csv'\ndf = pd.read_csv(file_path)\n\n# Convert the pandas DataFrame to a Hugging Face dataset\ndataset = Dataset.from_pandas(df)\n\n# Select a subset of the dataset for demonstration purposes (e.g., first 40 samples)\ndataset = dataset.select(range(48))\n\n# Define the function to format the chat template\ndef format_chat_template(row):\n    row_json = [{\"role\": \"user\", \"content\": row[\"User\"]},\n                {\"role\": \"assistant\", \"content\": row[\"Output\"]}]\n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\n# Apply the formatting function to the dataset\ndataset = dataset.map(\n    format_chat_template,\n    num_proc=4,\n)\n\n# Display the formatted text of the fourth sample\nprint(dataset['text'][3])\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T23:56:44.912535Z","iopub.execute_input":"2024-07-16T23:56:44.913374Z","iopub.status.idle":"2024-07-16T23:56:45.430328Z","shell.execute_reply.started":"2024-07-16T23:56:44.913342Z","shell.execute_reply":"2024-07-16T23:56:45.429142Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/48 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9bb3f166dae47da9ae8b879cb9bde61"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nNo chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\nNo chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\nNo chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\nNo chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n","output_type":"stream"},{"name":"stdout","text":"<|im_start|>user\nFEN_INPUT = r5rk/3nbp1p/pp1p1n2/7B/2q1PB2/P1N4R/1PQ5/1K5R b - - 1 1,\n    PGN_INPUT = [Variant \"From Position\"]\n[FEN \"r5rk/3nbp1p/pp1p1n2/7B/2q1PB2/P1N4R/1PQ5/1K1R4 w - - 0 1\"]\n\n1. Rdh1,\n    BEST_MOVE_INPUT = g8g7,\n    SEQUENCE_OF_BEST_MOVES_INPUT = ['g8g7', 'f4h6', 'g7g6', 'h5g6', 'f7g6', 'h6g5', 'a8g8', 'c3d5', 'c4c2', 'b1c2']\n    ALTERNATIVE_MOVE_1 = h8g7\n    ALTERNATIVE_MOVE_2 = c4d4\n    Explain why it is the best move, with a concise yet comprehensive explanation in points<|im_end|>\n<|im_start|>assistant\nHere's a comprehensive analysis of the best move g8g7:\n\n**Development and Control:**\n\n* g8g7 develops the kingside pieces, preparing to connect the rooks and potentially create a strong pawn center.\n* The move also aims to control key squares on the kingside, such as g6 and f6, which can become important outposts for the pieces.\n\n**Coordination:**\n\n* g8g7 improves the coordination between the king, queen, and rooks, allowing for potential attacks on the kingside and creating a more cohesive defensive structure.\n* The move prepares to potentially open the f-file for the rook, which can put pressure on White's position.\n\n**Tactical Insights:**\n\n* No immediate forks, pins, or skewers are created by g8g7, but it sets the stage for potential tactical opportunities in the future.\n* The move does not initiate a discovered attack, but it prepares to potentially create one with a future move like ...f5.\n\n**Defensive Strategy:**\n\n* g8g7 helps maintain a solid defensive posture by developing the kingside pieces and preparing to control key squares.\n* The move addresses the potential weakness on the kingside, making it more difficult for White to launch a successful attack.\n\n**Alternatives and Drawbacks:**\n\n* Alternative move h8g7:\n\t+ Weakens the kingside pawn structure, creating potential targets for White's pieces.\n\t+ Fails to develop the kingside pieces as effectively as g8g7.\n* Alternative move c4d4:\n\t+ Weakens the pawn structure in the center, creating potential targets for White's pieces.\n\t+ Fails to address the kingside development and control issues that g8g7 addresses.\n\n**Why g8g7 is the best move:**\n\n* g8g7 develops the kingside pieces, improves coordination, and prepares to control key squares, aligning with the opening and middle game principles.\n* The move sets the stage for potential tactical opportunities and maintains a solid defensive posture, making it a well-rounded and strategic choice.\n* Compared to alternative moves, g8g7 is a more comprehensive and effective way to address the position's challenges, making it the best move in this situation.<|im_end|>\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\n\n# Load the uploaded dataset\nfile_path = '/kaggle/input/finetuningchess/Fine_tuning_Dataset.csv'\ndf = pd.read_csv(file_path)\n\n# Drop the unnecessary column\ndf = df.drop(columns=[\"Unnamed: 0\"])\n\n# Convert the pandas DataFrame to a Hugging Face dataset\ndataset = Dataset.from_pandas(df)\n\n# Split the dataset into training and test sets\ndataset = dataset.train_test_split(test_size=0.2, seed=42)\ntrain_dataset = dataset['train']\ntest_dataset = dataset['test']\n\n# Initialize the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Define the function to format the chat template\ndef format_chat_template(row):\n    row_json = [{\"role\": \"user\", \"content\": row[\"User\"]},\n                {\"role\": \"assistant\", \"content\": row[\"Output\"]}]\n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\n# Apply the formatting function to the training and test sets\ntrain_dataset = train_dataset.map(format_chat_template, num_proc=4)\ntest_dataset = test_dataset.map(format_chat_template, num_proc=4)\n\n# Display the formatted text of the fourth sample in the training set\nprint(train_dataset['text'][3])\n\n# Display the formatted text of the fourth sample in the test set\nprint(test_dataset['text'][3])\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T23:56:47.592707Z","iopub.execute_input":"2024-07-16T23:56:47.593401Z","iopub.status.idle":"2024-07-16T23:56:48.693542Z","shell.execute_reply.started":"2024-07-16T23:56:47.593368Z","shell.execute_reply":"2024-07-16T23:56:48.692563Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/40 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eaa7c097778344579bb825bb49fcaf4b"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nNo chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\nNo chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\nNo chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\nNo chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c85681e0e4f54e15b8639205d8d3d2d0"}},"metadata":{}},{"name":"stderr","text":"No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\nNo chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\nNo chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\nNo chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n","output_type":"stream"},{"name":"stdout","text":"<|im_start|>user\nFEN_INPUT = 4q1k1/p5pp/2p5/p2p4/P2Pn3/1Pb4P/4QPP1/2B3K1 w - - 0 5,\n    PGN_INPUT = [Variant \"From Position\"]\n[FEN \"3qr1k1/p5pp/2pb1n2/p2p4/P7/1PNP3P/3Q1PP1/2B1R1K1 b - - 0 1\"]\n\n1... Bb4 2. Rxe8+ Qxe8 3. Bb2 (3. d4 Ne4 4. Qe2 Bxc3),\n    BEST_MOVE_INPUT = c1e3,\n    SEQUENCE_OF_BEST_MOVES_INPUT = ['c1e3', 'e4d6', 'e2a6', 'c3b4', 'g2g4', 'h7h5', 'g1h2', 'd6e4', 'a6d3', 'e8g6']\n    ALTERNATIVE_MOVE_1 = e2d3\n    ALTERNATIVE_MOVE_2 = e2d1\n    Explain why it is the best move, with a concise yet comprehensive explanation in points<|im_end|>\n<|im_start|>assistant\nHere's a comprehensive analysis of the best move c1e3:\n\n**Development and Control:**\n\n* c1e3 develops the bishop, putting it on a more active square and controlling the e-file.\n* It also prepares to develop other pieces, such as the queen and knight, by opening up the e-file and d-file.\n* Controlling the center with the bishop on e3 helps to restrict Black's mobility and creates potential weaknesses on the d-file.\n\n**Coordination:**\n\n* c1e3 improves the coordination between the bishop and queen, allowing them to work together to attack Black's position.\n* It also sets up a potential battery on the e-file, with the bishop and queen working together to attack Black's king.\n\n**Tactical Insights:**\n\n* **Pins:** c1e3 creates a potential pin on the knight against the queen, which can be exploited later.\n* **Discovered Attacks:** The move sets up a potential discovered attack on the knight, which can be initiated by moving the bishop or queen.\n\n**Defensive Strategy:**\n\n* c1e3 helps to maintain a solid defensive posture by controlling the center and preparing to develop other pieces.\n* It also addresses the potential weakness on the d-file, making it harder for Black to launch a successful attack.\n\n**Alternatives and Drawbacks:**\n\n* **e2d3:** This move develops the bishop, but it's slower and doesn't control the center as effectively as c1e3. It also doesn't set up the same level of coordination between the bishop and queen.\n* **e2d1:** This move develops the bishop, but it's even slower than e2d3 and doesn't address the weaknesses on the d-file.\n\n**Why c1e3 is the best move:**\n\n* It develops the bishop, controls the center, and sets up potential attacks on Black's position.\n* It improves coordination between the bishop and queen, creating a powerful attacking force.\n* It addresses potential weaknesses on the d-file and maintains a solid defensive posture.\n\nOverall, c1e3 is the best move because it develops the bishop, controls the center, and sets up potential attacks on Black's position, while also improving coordination and addressing potential weaknesses.<|im_end|>\n\n<|im_start|>user\nFEN_INPUT = 6k1/5rpp/Pq2b3/3pPp1P/5P2/8/2P3QK/R7 w - - 0 4,\n    PGN_INPUT = [Variant \"From Position\"]\n[FEN \"6k1/5rpp/PR2b3/3pPp1P/5P1q/2r4B/2P3QK/R7 b - - 0 1\"]\n\n1... Rxh3+ 2. Qxh3 Qf2+ 3. Qg2 Qxb6,\n    BEST_MOVE_INPUT = h5h6,\n    SEQUENCE_OF_BEST_MOVES_INPUT = ['h5h6', 'f7a7', 'g2g5', 'd5d4', 'g5h4', 'e6d5', 'a1g1', 'g7g6', 'g1d1', 'a7d7']\n    ALTERNATIVE_MOVE_1 = c2c3\n    ALTERNATIVE_MOVE_2 = a1a3\n    Explain why it is the best move, with a concise yet comprehensive explanation in points<|im_end|>\n<|im_start|>assistant\nHere's a comprehensive analysis of the best move h5h6:\n\n**Development and Control:**\n\n* h5h6 develops the pawn on h5, which was previously not contributing to the game.\n* It controls the f6 square, which is a key square in the center of the board, and prepares to potentially create a pawn chain in the future.\n* By playing h5h6, White is also eyeing the g7 square, which could become a weakness for Black if not addressed.\n\n**Coordination:**\n\n* h5h6 improves the coordination between the pawns on the kingside, creating a more cohesive pawn structure.\n* It also prepares to potentially open up the f-file for the rook on f1, which could become a powerful attacking force.\n\n**Tactical Insights:**\n\n* **Forks:** h5h6 does not create an immediate fork, but it sets up the potential for a fork on g7 in the future.\n* **Pins:** There are no immediate pins created by h5h6, but it prepares to potentially pin Black's pieces against the pawn on g7.\n* **Skewers:** There are no skewers created by h5h6.\n* **Discovered Attacks:** h5h6 does not initiate a discovered attack, but it prepares to potentially create one on the f-file.\n* **Double Attacks:** h5h6 does not create a double attack, but it sets up the potential for one on the f-file.\n\n**Defensive Strategy:**\n\n* h5h6 helps to maintain a solid defensive posture by controlling key squares and preparing to potentially create a pawn chain.\n* It also addresses the potential weakness on g7, which could have been exploited by Black.\n\n**Alternatives and Drawbacks:**\n\n* **c2c3:** This move develops the bishop, but it does not address the potential weakness on g7. It also does not contribute to controlling the center of the board.\n* **a1a3:** This move develops the bishop, but it is a slower move that does not address the immediate concerns on the kingside. It also does not contribute to controlling the center of the board.\n\n**Why h5h6 is the best move:**\n\n* h5h6 is the best move because it addresses the potential weakness on g7, develops the pawn on h5, and prepares to potentially create a pawn chain in the future.\n* It also improves the coordination between the pawns on the kingside and sets up the potential for a fork on g7.\n* Compared to the alternative moves, h5h6 is a more proactive move that addresses the immediate concerns on the kingside and contributes to controlling the center of the board.<|im_end|>\n\n","output_type":"stream"}]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    evaluation_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T23:56:48.695547Z","iopub.execute_input":"2024-07-16T23:56:48.695841Z","iopub.status.idle":"2024-07-16T23:56:48.726239Z","shell.execute_reply.started":"2024-07-16T23:56:48.695812Z","shell.execute_reply":"2024-07-16T23:56:48.725367Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from trl import SFTConfig, SFTTrainer\n\n\n\n# Setup the SFTTrainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    peft_config=peft_config,\n    max_seq_length=512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=False,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T23:56:48.727412Z","iopub.execute_input":"2024-07-16T23:56:48.727713Z","iopub.status.idle":"2024-07-16T23:56:48.963684Z","shell.execute_reply.started":"2024-07-16T23:56:48.727689Z","shell.execute_reply":"2024-07-16T23:56:48.962695Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff2cd981ac65458ba94bf8c077f7abc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"739aa0420ee549b1a8ebf6c0fc9c6fdf"}},"metadata":{}}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T23:56:48.966276Z","iopub.execute_input":"2024-07-16T23:56:48.966686Z","iopub.status.idle":"2024-07-16T23:59:52.491695Z","shell.execute_reply.started":"2024-07-16T23:56:48.966650Z","shell.execute_reply":"2024-07-16T23:59:52.490791Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [20/20 02:56, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>4</td>\n      <td>3.816500</td>\n      <td>3.835463</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>3.485600</td>\n      <td>3.172841</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>2.860600</td>\n      <td>2.664764</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>2.227100</td>\n      <td>2.323996</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.126600</td>\n      <td>2.156408</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/llama-3/transformers/8b-chat-hf/1 - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=20, training_loss=3.039326798915863, metrics={'train_runtime': 182.5777, 'train_samples_per_second': 0.219, 'train_steps_per_second': 0.11, 'total_flos': 927360111083520.0, 'train_loss': 3.039326798915863, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"wandb.finish()\nmodel.config.use_cache = True","metadata":{"execution":{"iopub.status.busy":"2024-07-17T00:03:25.161025Z","iopub.execute_input":"2024-07-17T00:03:25.161933Z","iopub.status.idle":"2024-07-17T00:03:31.642379Z","shell.execute_reply.started":"2024-07-17T00:03:25.161900Z","shell.execute_reply":"2024-07-17T00:03:31.641536Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▆▅▄▄▃▂▂▁▁</td></tr><tr><td>eval/runtime</td><td>▁▁▁▇▇▂██▇▇</td></tr><tr><td>eval/samples_per_second</td><td>███▁▁▅▁▁▁▁</td></tr><tr><td>eval/steps_per_second</td><td>███▁▁▅▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▇▇▇███▁▁▂▂▂▃▄▄▄▄▅▅▅▆▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▇▇▇███▁▁▂▂▂▃▄▄▄▄▅▅▅▆▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▁▁▁▁▁▂▃▅▆▅▄▆▄▃▃▂▂▃▂▂▂▂▁▂▄▆▄█▅▅▇▅▄▆▅▆▅▄▄▃</td></tr><tr><td>train/learning_rate</td><td>▂▂▃▄▅▅▆▇▇█▇▇▆▅▅▄▃▂▂▁▂▂▃▄▅▅▆▇▇█▇▇▆▅▅▄▃▂▂▁</td></tr><tr><td>train/loss</td><td>████▇▇▇▇▆▅▆▅▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.15641</td></tr><tr><td>eval/runtime</td><td>13.7674</td></tr><tr><td>eval/samples_per_second</td><td>0.726</td></tr><tr><td>eval/steps_per_second</td><td>0.726</td></tr><tr><td>total_flos</td><td>927360111083520.0</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>20</td></tr><tr><td>train/grad_norm</td><td>4.31609</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.1266</td></tr><tr><td>train_loss</td><td>3.03933</td></tr><tr><td>train_runtime</td><td>182.5777</td></tr><tr><td>train_samples_per_second</td><td>0.219</td></tr><tr><td>train_steps_per_second</td><td>0.11</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">deft-sun-1</strong> at: <a href='https://wandb.ai/krishnaharshith1234-bits-pilani/Fine-tune%20Llama%203%208B%20on%20chess%20dataset/runs/gzrcqg9p' target=\"_blank\">https://wandb.ai/krishnaharshith1234-bits-pilani/Fine-tune%20Llama%203%208B%20on%20chess%20dataset/runs/gzrcqg9p</a><br/> View project at: <a href='https://wandb.ai/krishnaharshith1234-bits-pilani/Fine-tune%20Llama%203%208B%20on%20chess%20dataset' target=\"_blank\">https://wandb.ai/krishnaharshith1234-bits-pilani/Fine-tune%20Llama%203%208B%20on%20chess%20dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240716_232242-gzrcqg9p/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."},"metadata":{}}]},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"\"\"\n        FEN_INPUT = 4r1k1/1pqnrpp1/p4n1p/P1pP4/R2bp3/1PN3P1/2PB1P1P/3QRBK1 w - - 1 2,\n        PGN_INPUT = [Variant From Position]\n        [FEN \"4r1k1/1pqnrpp1/p4n1p/P1pPb3/R3p3/1PN3P1/2PB1P1P/3QRBK1 b - - 0 1\"]\n\n        Bd4,\n        BEST_MOVE_INPUT = f1g2,\n        SEQUENCE_OF_BEST_MOVES_INPUT = ['f1g2', 'c7c8', 'e1e2', 'e7e5', 'd1e1', 'b7b5', 'a5b6', 'd7b6', 'a4a2', 'f6d5']\n        ALTERNATIVE_MOVE_1 = d2e3\n        ALTERNATIVE_MOVE_2 = f1h3\n        Explain why it is the best move, with a concise yet comprehensive explanation in points\n        \"\"\"\n    }\n]\n\n# Apply the chat template\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n# Prepare the inputs for the model\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\n# Generate the output\noutputs = model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_length=512, num_return_sequences=1)\n\n# Decode the generated text\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Print the generated response\nprint(text.split(\"assistant\")[1])","metadata":{"execution":{"iopub.status.busy":"2024-07-17T00:39:12.803332Z","iopub.execute_input":"2024-07-17T00:39:12.804074Z","iopub.status.idle":"2024-07-17T00:39:44.825202Z","shell.execute_reply.started":"2024-07-17T00:39:12.804041Z","shell.execute_reply":"2024-07-17T00:39:44.824203Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":" here's a comprehensive analysis of the best move ` : * * development and control : * * * ` : * * * * * coordination : * * * * ` : * * * * * * tactical insights : * * * * * * * ` : * * * * * * * * tactical and the center of the move ` : * * * * * * * * * tactical and the center of the move ` c7c8 ` : * * * * * * * * * * tactical and the center of the move ` c7c8 ` c7c8 ` c7c8 ` c7c8 ` c7c8 ` c7c8 ` c7c8 ` c7c8 ` c7c8 ` c7c8 ` c7c8 ` c7c8 ` c7c8 ` c7c8 ` c7c8 ` c7c8 ` c7c8 ` c7c8 ` c7c8 ` c7c8 ` c7c8 ` c7c8 ` c7c8 `\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}